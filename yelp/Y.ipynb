{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import EmpiricalMarginal, SVI, Trace_ELBO, JitTrace_ELBO\n",
    "import pandas as pd\n",
    "from torch.distributions import constraints\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "from pyro.infer.mcmc import NUTS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from preprocessor import YelpData\n",
    "from pyro.infer.autoguide import init_to_feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = YelpData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for first time \n",
    "# data_class.process()\n",
    "# data = torch.from_numpy(data_class.add_bias().astype(np.float32))\n",
    "# ratings = torch.from_numpy(data_class.ratings.to_numpy().astype(np.float32))\n",
    "# data_class.to_pickle(data, 'data')\n",
    "# data_class.to_pickle(ratings, 'ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_class.load('data')/ 10000\n",
    "ratings = data_class.load('ratings')\n",
    "num_features = data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, ratings):\n",
    "#     betas = []\n",
    "\n",
    "    #bias\n",
    "#     betas.append(pyro.sample(\"beta_0\", dist.Normal(0, 1)))\n",
    "    #make one-hot vector length betas\n",
    "    with pyro.plate(\"betas\", num_features + 1):\n",
    "        betas = pyro.sample(\"beta\", dist.Gamma(1,1))\n",
    "#     for i in range(num_feature):\n",
    "#         betas.append(pyro.sample(\"beta_{}\".format(i + 1), dist.Normal(0, 1)))\n",
    "\n",
    "#     betas = torch.from_numpy(np.array(betas))\n",
    "#     print(betas.dtype)\n",
    "    lambda_ = torch.exp(torch.sum(betas * data))\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    with pyro.plate(\"ratings\", ratings.shape[0]):\n",
    "        y = pyro.sample(\"obs\", dist.Poisson(lambda_), obs = ratings)\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    return y\n",
    "# model(data, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(data, ratings):\n",
    "    alphas_0 = pyro.param('weights_loc', torch.ones(546),  constraint=constraints.positive)\n",
    "    alphas_1 = pyro.param('weights_scale', torch.ones(546), constraint=constraints.positive)        \n",
    "#     weights_loc = pyro.param('weights_loc', torch.randn(2))\n",
    "#     weights_scale = pyro.param('weights_scale', torch.ones(2), constraint=constraints.positive)        \n",
    "\n",
    "    with pyro.plate(\"betas\", num_features + 1):\n",
    "        betas = pyro.sample(\"beta\", dist.Gamma(alphas_0, alphas_1))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def guide(rating, rating_ns, popularity):\n",
    "#     weights_loc = pyro.param('weights_loc', torch.randn(3))\n",
    "#     weights_scale = pyro.param('weights_scale', torch.ones(3), constraint=constraints.positive)        \n",
    "# #     weights_loc = pyro.param('weights_loc', torch.randn(2))\n",
    "# #     weights_scale = pyro.param('weights_scale', torch.ones(2), constraint=constraints.positive)        \n",
    "\n",
    "    \n",
    "#     beta_0 = pyro.sample(\"beta_0\", dist.Normal(weights_loc[0], weights_scale[0]))\n",
    "#     beta_1 = pyro.sample(\"beta_1\", dist.Normal(weights_loc[1], weights_scale[1]))\n",
    "#     beta_2 = pyro.sample(\"beta_2\", dist.Normal(weights_loc[2], weights_scale[2]))\n",
    "#     labmda_ = torch.exp(beta_0 + beta_1 * rating + beta_2 * rating_ns)\n",
    "# #     labmda_ = torch.exp(beta_0 + beta_1 * rating)\n",
    "\n",
    "# guide = pyro.infer.autoguide.AutoDiagonalNormal(model, init_loc_fn=init_to_feasible)\n",
    "# guide = pyro.infer.autoguide.AutoMultivariateNormal(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70459280.0\n",
      "14424645.0\n",
      "383846.0\n",
      "6101869.0\n",
      "817356.4375\n",
      "1067403.5\n",
      "206303.90625\n",
      "1287025.0\n",
      "607308.5\n",
      "378354.375\n",
      "1997951.5\n",
      "2112041.0\n",
      "1875310.75\n",
      "2056038.75\n",
      "467686.6875\n",
      "88704.6875\n",
      "127718.078125\n",
      "73377.96875\n",
      "124374.40625\n",
      "185939.46875\n"
     ]
    }
   ],
   "source": [
    "svi = SVI(model, \n",
    "          guide, \n",
    "          optim.Adam({\"lr\": .005}), \n",
    "          loss=JitTrace_ELBO(), \n",
    "          num_samples=1000)\n",
    "\n",
    "# pyro.clear_param_store()\n",
    "epoch = 10000\n",
    "for i in range(epoch):\n",
    "    ELBO = svi.step(data, ratings)\n",
    "    if i % 500 == 0:\n",
    "        print(ELBO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1200/1200 [3:03:59<00:00,  9.75s/it, step size=2.34e-02, acc. prob=0.922] \n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(model)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=200)\n",
    "mcmc.run(data, ratings)\n",
    "\n",
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_class.to_pickle(hmc_samples, 'hmc_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to print latent sites' quantile information.\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for site_name, values in samples.items():\n",
    "        marginal_site = pd.DataFrame(values)\n",
    "        describe = marginal_site.describe(percentiles=[.05, 0.25, 0.5, 0.75, 0.95]).transpose()\n",
    "        site_stats[site_name] = describe[[\"mean\", \"std\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\"]]\n",
    "    return site_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: betas\n",
      "      mean  std     5%    25%    50%    75%    95%\n",
      "0      0.0  0.0    0.0    0.0    0.0    0.0    0.0\n",
      "1      1.0  0.0    1.0    1.0    1.0    1.0    1.0\n",
      "2      2.0  0.0    2.0    2.0    2.0    2.0    2.0\n",
      "3      3.0  0.0    3.0    3.0    3.0    3.0    3.0\n",
      "4      4.0  0.0    4.0    4.0    4.0    4.0    4.0\n",
      "..     ...  ...    ...    ...    ...    ...    ...\n",
      "541  541.0  0.0  541.0  541.0  541.0  541.0  541.0\n",
      "542  542.0  0.0  542.0  542.0  542.0  542.0  542.0\n",
      "543  543.0  0.0  543.0  543.0  543.0  543.0  543.0\n",
      "544  544.0  0.0  544.0  544.0  544.0  544.0  544.0\n",
      "545  545.0  0.0  545.0  545.0  545.0  545.0  545.0\n",
      "\n",
      "[546 rows x 7 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "svi_diagnorm_posterior = svi.run(data,ratings)\n",
    "sites = [\"betas\"]\n",
    "\n",
    "svi_samples = {site: EmpiricalMarginal(svi_diagnorm_posterior, sites=site)\n",
    "                     .enumerate_support().detach().cpu().numpy()\n",
    "               for site in sites}\n",
    "for site, values in summary(svi_samples).items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(\"Posterior Distribution\", fontsize=16)\n",
    "for i, ax in enumerate(axs.reshape(-1)):\n",
    "    try:\n",
    "        site = sites[i]\n",
    "        sns.distplot(hmc_samples[site], ax=ax, label=\"HMC\")        \n",
    "        sns.distplot(svi_samples[site], ax=ax, label=\"SVI\")\n",
    "        ax.set_title(site)\n",
    "    except:\n",
    "        pass\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MLE\n",
    "\n",
    "# #variables\n",
    "# MLE_thetas = torch.ones(train_data.shape[1] - 1, requires_grad = True)\n",
    "# def neg_log_likelihood(x,y):\n",
    "#     theta_times_x = MLE_thetas * x\n",
    "#     log_likelihood = torch.sum(y * theta_times_x - torch.exp(theta_times_x))\n",
    "#     return -log_likelihood\n",
    "\n",
    "# def get_MLE(x, y):\n",
    "#     epoch = 10000\n",
    "#     optimizer = torch.optim.Adam([MLE_thetas])\n",
    "#     for i in range(epoch):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         target = neg_log_likelihood(x,y)\n",
    "#         target.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# get_MLE(train_data[:,:2],train_data[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLE\n",
    "\n",
    "#variables\n",
    "MLE_thetas = torch.ones([548,1], requires_grad = True, dtype = torch.float32)\n",
    "def neg_log_likelihood(x,y):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    theta_times_x = torch.matmul(x, MLE_thetas)\n",
    "\n",
    "\n",
    "    log_likelihood = torch.matmul(y, theta_times_x) + torch.sum(torch.exp(theta_times_x))\n",
    "#     torch.sum(y * theta_times_x - torch.exp(theta_times_x))\n",
    "    return -log_likelihood\n",
    "\n",
    "def get_MLE(x, y):\n",
    "    epoch = 1000\n",
    "    optimizer = torch.optim.Adam([MLE_thetas])\n",
    "    for i in range(epoch):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        target = neg_log_likelihood(x,y)\n",
    "#         print(target)\n",
    "        target.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "get_MLE(data, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5136],\n",
       "        [4.2317],\n",
       "        [3.7128],\n",
       "        [2.7420],\n",
       "        [1.2363],\n",
       "        [4.0657],\n",
       "        [4.3124],\n",
       "        [3.5248],\n",
       "        [4.0266],\n",
       "        [1.2363],\n",
       "        [3.0732],\n",
       "        [3.2069],\n",
       "        [3.5548],\n",
       "        [1.2363],\n",
       "        [3.9610],\n",
       "        [4.2634],\n",
       "        [4.3900],\n",
       "        [3.5367],\n",
       "        [4.2208],\n",
       "        [4.1062],\n",
       "        [4.2268],\n",
       "        [2.2122],\n",
       "        [4.3321],\n",
       "        [3.9244],\n",
       "        [3.9843],\n",
       "        [4.3715],\n",
       "        [3.9043],\n",
       "        [4.1201],\n",
       "        [2.2122],\n",
       "        [3.2042],\n",
       "        [3.0741],\n",
       "        [4.5127],\n",
       "        [3.0742],\n",
       "        [4.0018],\n",
       "        [3.3578],\n",
       "        [4.1247],\n",
       "        [3.5959],\n",
       "        [4.2773],\n",
       "        [4.3532],\n",
       "        [4.1354],\n",
       "        [1.2363],\n",
       "        [2.3087],\n",
       "        [2.3087],\n",
       "        [4.1038],\n",
       "        [4.0225],\n",
       "        [3.9273],\n",
       "        [4.3853],\n",
       "        [3.9795],\n",
       "        [3.9377],\n",
       "        [3.3540],\n",
       "        [3.1862],\n",
       "        [3.8257],\n",
       "        [2.7416],\n",
       "        [4.2225],\n",
       "        [2.7477],\n",
       "        [3.6237],\n",
       "        [4.5200],\n",
       "        [4.2299],\n",
       "        [1.2363],\n",
       "        [3.7014],\n",
       "        [4.1780],\n",
       "        [4.1721],\n",
       "        [1.2363],\n",
       "        [4.4213],\n",
       "        [3.7892],\n",
       "        [3.2070],\n",
       "        [4.2970],\n",
       "        [4.2223],\n",
       "        [3.8379],\n",
       "        [3.9929],\n",
       "        [4.2683],\n",
       "        [3.9755],\n",
       "        [4.2758],\n",
       "        [4.2649],\n",
       "        [4.1171],\n",
       "        [4.3804],\n",
       "        [3.4015],\n",
       "        [2.2122],\n",
       "        [4.1368],\n",
       "        [3.1378],\n",
       "        [2.3084],\n",
       "        [4.1519],\n",
       "        [4.0646],\n",
       "        [1.2363],\n",
       "        [3.9871],\n",
       "        [4.1291],\n",
       "        [3.9365],\n",
       "        [4.0811],\n",
       "        [3.9921],\n",
       "        [3.9921],\n",
       "        [3.7405],\n",
       "        [2.2122],\n",
       "        [3.9408],\n",
       "        [4.2773],\n",
       "        [2.2122],\n",
       "        [3.2981],\n",
       "        [3.6763],\n",
       "        [3.9691],\n",
       "        [1.2363],\n",
       "        [4.2213],\n",
       "        [2.2122],\n",
       "        [4.4035],\n",
       "        [3.3359],\n",
       "        [1.2363],\n",
       "        [4.0653],\n",
       "        [3.8607],\n",
       "        [3.9876],\n",
       "        [3.5908],\n",
       "        [2.7394],\n",
       "        [3.8760],\n",
       "        [3.9945],\n",
       "        [4.4410],\n",
       "        [4.2773],\n",
       "        [4.2315],\n",
       "        [3.0741],\n",
       "        [2.3085],\n",
       "        [2.3087],\n",
       "        [3.8449],\n",
       "        [2.2122],\n",
       "        [3.9977],\n",
       "        [3.0345],\n",
       "        [3.6764],\n",
       "        [4.3770],\n",
       "        [3.5999],\n",
       "        [4.0573],\n",
       "        [3.4804],\n",
       "        [3.9921],\n",
       "        [4.1656],\n",
       "        [4.0764],\n",
       "        [4.1988],\n",
       "        [4.0803],\n",
       "        [4.1645],\n",
       "        [3.7333],\n",
       "        [4.2649],\n",
       "        [3.2035],\n",
       "        [4.0594],\n",
       "        [3.1102],\n",
       "        [4.0286],\n",
       "        [4.1054],\n",
       "        [4.2127],\n",
       "        [4.4103],\n",
       "        [4.1748],\n",
       "        [3.1653],\n",
       "        [3.9403],\n",
       "        [4.3563],\n",
       "        [3.9655],\n",
       "        [4.1916],\n",
       "        [3.6842],\n",
       "        [3.9697],\n",
       "        [4.3157],\n",
       "        [3.9895],\n",
       "        [4.2970],\n",
       "        [3.6550],\n",
       "        [4.0745],\n",
       "        [1.2363],\n",
       "        [4.2721],\n",
       "        [3.7403],\n",
       "        [2.2122],\n",
       "        [2.2122],\n",
       "        [3.0206],\n",
       "        [3.8372],\n",
       "        [3.7813],\n",
       "        [3.3578],\n",
       "        [3.7347],\n",
       "        [3.5036],\n",
       "        [2.6761],\n",
       "        [3.0005],\n",
       "        [3.4696],\n",
       "        [3.1742],\n",
       "        [3.9929],\n",
       "        [4.4275],\n",
       "        [4.4415],\n",
       "        [4.0641],\n",
       "        [3.3240],\n",
       "        [1.2363],\n",
       "        [4.0672],\n",
       "        [4.0516],\n",
       "        [4.0885],\n",
       "        [4.3375],\n",
       "        [4.0831],\n",
       "        [4.0004],\n",
       "        [3.6743],\n",
       "        [4.3587],\n",
       "        [3.7463],\n",
       "        [2.7593],\n",
       "        [4.0567],\n",
       "        [2.3086],\n",
       "        [4.0192],\n",
       "        [3.6764],\n",
       "        [4.1842],\n",
       "        [4.2682],\n",
       "        [4.1148],\n",
       "        [3.9913],\n",
       "        [4.0293],\n",
       "        [4.3475],\n",
       "        [3.4800],\n",
       "        [4.1721],\n",
       "        [4.3622],\n",
       "        [3.5918],\n",
       "        [2.3081],\n",
       "        [4.0636],\n",
       "        [3.0741],\n",
       "        [4.3243],\n",
       "        [3.9325],\n",
       "        [2.9718],\n",
       "        [4.5138],\n",
       "        [3.4448],\n",
       "        [3.3213],\n",
       "        [1.2363],\n",
       "        [2.9528],\n",
       "        [4.0678],\n",
       "        [3.1420],\n",
       "        [3.6673],\n",
       "        [3.8729],\n",
       "        [4.2460],\n",
       "        [3.4263],\n",
       "        [4.3935],\n",
       "        [3.0575],\n",
       "        [3.8581],\n",
       "        [3.8407],\n",
       "        [3.5733],\n",
       "        [4.0406],\n",
       "        [3.8562],\n",
       "        [3.6764],\n",
       "        [3.3166],\n",
       "        [4.0393],\n",
       "        [4.3429],\n",
       "        [2.6761],\n",
       "        [4.0877],\n",
       "        [4.1766],\n",
       "        [4.1841],\n",
       "        [4.4335],\n",
       "        [4.2897],\n",
       "        [3.1649],\n",
       "        [3.0744],\n",
       "        [3.2915],\n",
       "        [4.4415],\n",
       "        [3.7427],\n",
       "        [1.2363],\n",
       "        [1.2363],\n",
       "        [3.0698],\n",
       "        [4.0510],\n",
       "        [4.3638],\n",
       "        [3.1518],\n",
       "        [3.9273],\n",
       "        [3.9004],\n",
       "        [4.0716],\n",
       "        [3.8712],\n",
       "        [3.9917],\n",
       "        [3.0150],\n",
       "        [1.2363],\n",
       "        [4.3713],\n",
       "        [4.3080],\n",
       "        [4.1712],\n",
       "        [2.3081],\n",
       "        [1.2363],\n",
       "        [3.9274],\n",
       "        [3.9409],\n",
       "        [3.9153],\n",
       "        [4.0834],\n",
       "        [3.0768],\n",
       "        [4.2555],\n",
       "        [3.0715],\n",
       "        [3.0047],\n",
       "        [4.0171],\n",
       "        [3.9890],\n",
       "        [1.2363],\n",
       "        [1.2363],\n",
       "        [3.9442],\n",
       "        [4.2710],\n",
       "        [2.2122],\n",
       "        [4.3432],\n",
       "        [3.6407],\n",
       "        [3.9550],\n",
       "        [4.1818],\n",
       "        [3.9348],\n",
       "        [1.2363],\n",
       "        [3.3293],\n",
       "        [3.3540],\n",
       "        [3.6255],\n",
       "        [4.4665],\n",
       "        [4.0770],\n",
       "        [4.3175],\n",
       "        [3.6395],\n",
       "        [4.2800],\n",
       "        [3.6172],\n",
       "        [3.5908],\n",
       "        [4.4023],\n",
       "        [4.2973],\n",
       "        [2.3082],\n",
       "        [2.2122],\n",
       "        [4.2551],\n",
       "        [4.2785],\n",
       "        [3.3578],\n",
       "        [4.1667],\n",
       "        [3.8647],\n",
       "        [4.1519],\n",
       "        [3.9030],\n",
       "        [3.2460],\n",
       "        [4.2766],\n",
       "        [3.7492],\n",
       "        [4.3821],\n",
       "        [4.3480],\n",
       "        [1.2363],\n",
       "        [4.0842],\n",
       "        [3.5908],\n",
       "        [4.0588],\n",
       "        [2.2122],\n",
       "        [4.3049],\n",
       "        [4.2016],\n",
       "        [3.6764],\n",
       "        [4.2831],\n",
       "        [3.6138],\n",
       "        [3.7741],\n",
       "        [4.4919],\n",
       "        [3.6584],\n",
       "        [3.5312],\n",
       "        [3.2340],\n",
       "        [4.2759],\n",
       "        [3.9767],\n",
       "        [2.7468],\n",
       "        [4.2300],\n",
       "        [4.2997],\n",
       "        [3.9521],\n",
       "        [2.7386],\n",
       "        [4.0900],\n",
       "        [2.3086],\n",
       "        [3.9837],\n",
       "        [4.5206],\n",
       "        [4.4665],\n",
       "        [4.2317],\n",
       "        [3.2449],\n",
       "        [3.7427],\n",
       "        [2.3083],\n",
       "        [3.5940],\n",
       "        [3.8479],\n",
       "        [3.4152],\n",
       "        [2.7396],\n",
       "        [1.2363],\n",
       "        [3.1381],\n",
       "        [4.2755],\n",
       "        [3.9161],\n",
       "        [1.2363],\n",
       "        [3.8027],\n",
       "        [3.8379],\n",
       "        [3.9940],\n",
       "        [4.1712],\n",
       "        [1.2363],\n",
       "        [3.9342],\n",
       "        [4.4035],\n",
       "        [3.2065],\n",
       "        [4.2773],\n",
       "        [3.9153],\n",
       "        [1.2363],\n",
       "        [3.4729],\n",
       "        [4.0016],\n",
       "        [3.9909],\n",
       "        [1.2363],\n",
       "        [3.6064],\n",
       "        [4.1846],\n",
       "        [3.0744],\n",
       "        [3.0706],\n",
       "        [4.1269],\n",
       "        [3.2516],\n",
       "        [3.5846],\n",
       "        [3.9565],\n",
       "        [3.6002],\n",
       "        [3.0706],\n",
       "        [3.8327],\n",
       "        [3.6764],\n",
       "        [4.3618],\n",
       "        [3.8975],\n",
       "        [2.7522],\n",
       "        [4.2208],\n",
       "        [4.4924],\n",
       "        [2.3079],\n",
       "        [2.3081],\n",
       "        [1.2363],\n",
       "        [3.9929],\n",
       "        [4.4141],\n",
       "        [3.4719],\n",
       "        [2.3087],\n",
       "        [3.2060],\n",
       "        [4.1931],\n",
       "        [4.0651],\n",
       "        [3.6236],\n",
       "        [3.2449],\n",
       "        [4.1886],\n",
       "        [4.0286],\n",
       "        [3.9569],\n",
       "        [4.3699],\n",
       "        [4.1721],\n",
       "        [3.6749],\n",
       "        [3.7347],\n",
       "        [3.8673],\n",
       "        [4.2390],\n",
       "        [4.0715],\n",
       "        [4.3617],\n",
       "        [4.1258],\n",
       "        [4.4038],\n",
       "        [3.9929],\n",
       "        [3.7361],\n",
       "        [1.2363],\n",
       "        [4.2649],\n",
       "        [4.0920],\n",
       "        [3.1410],\n",
       "        [4.1558],\n",
       "        [3.5455],\n",
       "        [4.0798],\n",
       "        [3.7326],\n",
       "        [3.7515],\n",
       "        [3.0739],\n",
       "        [4.3984],\n",
       "        [4.3507],\n",
       "        [3.7339],\n",
       "        [3.6271],\n",
       "        [1.2363],\n",
       "        [3.7865],\n",
       "        [4.4674],\n",
       "        [3.8379],\n",
       "        [3.8375],\n",
       "        [4.0834],\n",
       "        [4.3194],\n",
       "        [4.1407],\n",
       "        [3.2979],\n",
       "        [3.5352],\n",
       "        [4.1225],\n",
       "        [3.5836],\n",
       "        [4.0192],\n",
       "        [4.3175],\n",
       "        [1.2363],\n",
       "        [4.1028],\n",
       "        [4.2841],\n",
       "        [2.7584],\n",
       "        [2.9856],\n",
       "        [2.2122],\n",
       "        [4.0601],\n",
       "        [3.1469],\n",
       "        [4.1614],\n",
       "        [4.3431],\n",
       "        [3.0137],\n",
       "        [3.7463],\n",
       "        [3.9190],\n",
       "        [1.2363],\n",
       "        [3.5658],\n",
       "        [4.3852],\n",
       "        [4.4053],\n",
       "        [1.2363],\n",
       "        [4.1946],\n",
       "        [4.1341],\n",
       "        [3.9921],\n",
       "        [3.8306],\n",
       "        [4.1153],\n",
       "        [4.0185],\n",
       "        [4.3093],\n",
       "        [4.3925],\n",
       "        [3.6319],\n",
       "        [3.5061],\n",
       "        [3.6911],\n",
       "        [3.9847],\n",
       "        [4.0623],\n",
       "        [4.0348],\n",
       "        [3.5724],\n",
       "        [4.1179],\n",
       "        [4.2501],\n",
       "        [3.1755],\n",
       "        [4.2314],\n",
       "        [3.5888],\n",
       "        [3.9921],\n",
       "        [3.9118],\n",
       "        [1.2363],\n",
       "        [4.2946],\n",
       "        [4.2645],\n",
       "        [1.2363],\n",
       "        [4.2773],\n",
       "        [3.6171],\n",
       "        [3.3943],\n",
       "        [4.0761],\n",
       "        [3.6310],\n",
       "        [3.1606],\n",
       "        [3.8379],\n",
       "        [4.2970],\n",
       "        [3.8647],\n",
       "        [3.9624],\n",
       "        [4.0041],\n",
       "        [4.0879],\n",
       "        [3.6764],\n",
       "        [3.6754],\n",
       "        [3.0744],\n",
       "        [4.1150],\n",
       "        [1.2363],\n",
       "        [3.5891],\n",
       "        [3.6764],\n",
       "        [4.2649],\n",
       "        [2.2122],\n",
       "        [4.4186],\n",
       "        [3.8861],\n",
       "        [4.2410],\n",
       "        [1.2363],\n",
       "        [4.2679],\n",
       "        [2.7575],\n",
       "        [4.2570],\n",
       "        [3.1725],\n",
       "        [4.1805],\n",
       "        [3.8347],\n",
       "        [4.1783],\n",
       "        [1.2363],\n",
       "        [3.6764],\n",
       "        [2.7584],\n",
       "        [4.3040],\n",
       "        [3.0963],\n",
       "        [3.7785],\n",
       "        [3.7250],\n",
       "        [4.1852],\n",
       "        [4.3187],\n",
       "        [4.2876],\n",
       "        [4.1538],\n",
       "        [4.1231],\n",
       "        [2.7382],\n",
       "        [3.9408],\n",
       "        [3.9289],\n",
       "        [4.1938],\n",
       "        [2.3081],\n",
       "        [4.4025],\n",
       "        [4.2773],\n",
       "        [4.2773],\n",
       "        [2.3086],\n",
       "        [4.2758],\n",
       "        [4.4388],\n",
       "        [4.0868],\n",
       "        [4.0570],\n",
       "        [3.8373],\n",
       "        [1.2363],\n",
       "        [4.0786],\n",
       "        [4.3270],\n",
       "        [2.2122],\n",
       "        [4.0091],\n",
       "        [3.8385],\n",
       "        [3.8013],\n",
       "        [3.3712],\n",
       "        [4.0532],\n",
       "        [4.3535],\n",
       "        [3.6170],\n",
       "        [4.2522],\n",
       "        [4.2206],\n",
       "        [4.1247],\n",
       "        [3.4630],\n",
       "        [1.2363]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLE_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
