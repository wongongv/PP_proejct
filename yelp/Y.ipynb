{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import EmpiricalMarginal, SVI, Trace_ELBO, JitTrace_ELBO\n",
    "import pandas as pd\n",
    "from torch.distributions import constraints\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "from pyro.infer.mcmc import NUTS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from preprocessor import YelpData\n",
    "from pyro.infer.autoguide import init_to_feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = YelpData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for first time \n",
    "data_class.process()\n",
    "data = torch.from_numpy(data_class.add_bias().astype(np.float32))\n",
    "ratings = torch.from_numpy(data_class.ratings.to_numpy().astype(np.float32))\n",
    "data_class.to_pickle(data, 'data')\n",
    "data_class.to_pickle(ratings, 'ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_class.load('data')/ 10000\n",
    "ratings = data_class.load('ratings')\n",
    "num_features = data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, ratings):\n",
    "#     betas = []\n",
    "\n",
    "    #bias\n",
    "#     betas.append(pyro.sample(\"beta_0\", dist.Normal(0, 1)))\n",
    "    #make one-hot vector length betas\n",
    "    with pyro.plate(\"betas\", num_features + 1):\n",
    "        betas = pyro.sample(\"beta\", dist.Gamma(1,1))\n",
    "#     for i in range(num_feature):\n",
    "#         betas.append(pyro.sample(\"beta_{}\".format(i + 1), dist.Normal(0, 1)))\n",
    "\n",
    "#     betas = torch.from_numpy(np.array(betas))\n",
    "#     print(betas.dtype)\n",
    "    lambda_ = torch.exp(torch.sum(betas * data))\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    with pyro.plate(\"ratings\", ratings.shape[0]):\n",
    "        y = pyro.sample(\"obs\", dist.Poisson(lambda_), obs = ratings)\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    return y\n",
    "# model(data, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(data, ratings):\n",
    "    alphas_0 = pyro.param('weights_loc', torch.ones(546),  constraint=constraints.positive)\n",
    "    alphas_1 = pyro.param('weights_scale', torch.ones(546), constraint=constraints.positive)        \n",
    "#     weights_loc = pyro.param('weights_loc', torch.randn(2))\n",
    "#     weights_scale = pyro.param('weights_scale', torch.ones(2), constraint=constraints.positive)        \n",
    "\n",
    "    with pyro.plate(\"betas\", num_features + 1):\n",
    "        betas = pyro.sample(\"beta\", dist.Gamma(alphas_0, alphas_1))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def guide(rating, rating_ns, popularity):\n",
    "#     weights_loc = pyro.param('weights_loc', torch.randn(3))\n",
    "#     weights_scale = pyro.param('weights_scale', torch.ones(3), constraint=constraints.positive)        \n",
    "# #     weights_loc = pyro.param('weights_loc', torch.randn(2))\n",
    "# #     weights_scale = pyro.param('weights_scale', torch.ones(2), constraint=constraints.positive)        \n",
    "\n",
    "    \n",
    "#     beta_0 = pyro.sample(\"beta_0\", dist.Normal(weights_loc[0], weights_scale[0]))\n",
    "#     beta_1 = pyro.sample(\"beta_1\", dist.Normal(weights_loc[1], weights_scale[1]))\n",
    "#     beta_2 = pyro.sample(\"beta_2\", dist.Normal(weights_loc[2], weights_scale[2]))\n",
    "#     labmda_ = torch.exp(beta_0 + beta_1 * rating + beta_2 * rating_ns)\n",
    "# #     labmda_ = torch.exp(beta_0 + beta_1 * rating)\n",
    "\n",
    "# guide = pyro.infer.autoguide.AutoDiagonalNormal(model, init_loc_fn=init_to_feasible)\n",
    "# guide = pyro.infer.autoguide.AutoMultivariateNormal(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70459280.0\n",
      "14424645.0\n",
      "383846.0\n",
      "6101869.0\n",
      "817356.4375\n",
      "1067403.5\n",
      "206303.90625\n",
      "1287025.0\n",
      "607308.5\n",
      "378354.375\n",
      "1997951.5\n",
      "2112041.0\n",
      "1875310.75\n",
      "2056038.75\n",
      "467686.6875\n",
      "88704.6875\n",
      "127718.078125\n",
      "73377.96875\n",
      "124374.40625\n",
      "185939.46875\n"
     ]
    }
   ],
   "source": [
    "svi = SVI(model, \n",
    "          guide, \n",
    "          optim.Adam({\"lr\": .005}), \n",
    "          loss=JitTrace_ELBO(), \n",
    "          num_samples=1000)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "epoch = 10000\n",
    "for i in range(epoch):\n",
    "    ELBO = svi.step(data, ratings)\n",
    "    if i % 500 == 0:\n",
    "        print(ELBO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample:  71%|███████▏  | 856/1200 [2:11:04<56:06,  9.79s/it, step size=2.34e-02, acc. prob=0.920]  "
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(model)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=200)\n",
    "mcmc.run(data, ratings)\n",
    "\n",
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to print latent sites' quantile information.\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for site_name, values in samples.items():\n",
    "        marginal_site = pd.DataFrame(values)\n",
    "        describe = marginal_site.describe(percentiles=[.05, 0.25, 0.5, 0.75, 0.95]).transpose()\n",
    "        site_stats[site_name] = describe[[\"mean\", \"std\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\"]]\n",
    "    return site_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svi_diagnorm_posterior = svi.run(data,ratings)\n",
    "sites = [\"beta_0\", \"beta_1\"]\n",
    "\n",
    "svi_samples = {site: EmpiricalMarginal(svi_diagnorm_posterior, sites=site)\n",
    "                     .enumerate_support().detach().cpu().numpy()\n",
    "               for site in sites}\n",
    "for site, values in summary(svi_samples).items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=````````````````````````````````````````````````3, figsize=(18, 5))\n",
    "fig.suptitle(\"Posterior Distribution\", fontsize=16)\n",
    "for i, ax in enumerate(axs.reshape(-1)):\n",
    "    try:\n",
    "        site = sites[i]\n",
    "        sns.distplot(hmc_samples[site], ax=ax, label=\"HMC\")        \n",
    "        sns.distplot(svi_samples[site], ax=ax, label=\"SVI\")\n",
    "        ax.set_title(site)\n",
    "    except:\n",
    "        pass\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MLE\n",
    "\n",
    "# #variables\n",
    "# MLE_thetas = torch.ones(train_data.shape[1] - 1, requires_grad = True)\n",
    "# def neg_log_likelihood(x,y):\n",
    "#     theta_times_x = MLE_thetas * x\n",
    "#     log_likelihood = torch.sum(y * theta_times_x - torch.exp(theta_times_x))\n",
    "#     return -log_likelihood\n",
    "\n",
    "# def get_MLE(x, y):\n",
    "#     epoch = 10000\n",
    "#     optimizer = torch.optim.Adam([MLE_thetas])\n",
    "#     for i in range(epoch):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         target = neg_log_likelihood(x,y)\n",
    "#         target.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# get_MLE(train_data[:,:2],train_data[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLE\n",
    "\n",
    "#variables\n",
    "MLE_thetas = torch.ones([548,1], requires_grad = True, dtype = torch.float32)\n",
    "def neg_log_likelihood(x,y):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    theta_times_x = torch.matmul(x, MLE_thetas)\n",
    "\n",
    "\n",
    "    log_likelihood = torch.matmul(y, theta_times_x) + torch.sum(torch.exp(theta_times_x))\n",
    "#     torch.sum(y * theta_times_x - torch.exp(theta_times_x))\n",
    "    return -log_likelihood\n",
    "\n",
    "def get_MLE(x, y):\n",
    "    epoch = 1000\n",
    "    optimizer = torch.optim.Adam([MLE_thetas])\n",
    "    for i in range(epoch):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        target = neg_log_likelihood(x,y)\n",
    "#         print(target)\n",
    "        target.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "get_MLE(data, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5136],\n",
       "        [4.2317],\n",
       "        [3.7128],\n",
       "        [2.7420],\n",
       "        [1.2363],\n",
       "        [4.0657],\n",
       "        [4.3124],\n",
       "        [3.5248],\n",
       "        [4.0266],\n",
       "        [1.2363],\n",
       "        [3.0732],\n",
       "        [3.2069],\n",
       "        [3.5548],\n",
       "        [1.2363],\n",
       "        [3.9610],\n",
       "        [4.2634],\n",
       "        [4.3900],\n",
       "        [3.5367],\n",
       "        [4.2208],\n",
       "        [4.1062],\n",
       "        [4.2268],\n",
       "        [2.2122],\n",
       "        [4.3321],\n",
       "        [3.9244],\n",
       "        [3.9843],\n",
       "        [4.3715],\n",
       "        [3.9043],\n",
       "        [4.1201],\n",
       "        [2.2122],\n",
       "        [3.2042],\n",
       "        [3.0741],\n",
       "        [4.5127],\n",
       "        [3.0742],\n",
       "        [4.0018],\n",
       "        [3.3578],\n",
       "        [4.1247],\n",
       "        [3.5959],\n",
       "        [4.2773],\n",
       "        [4.3532],\n",
       "        [4.1354],\n",
       "        [1.2363],\n",
       "        [2.3087],\n",
       "        [2.3087],\n",
       "        [4.1038],\n",
       "        [4.0225],\n",
       "        [3.9273],\n",
       "        [4.3853],\n",
       "        [3.9795],\n",
       "        [3.9377],\n",
       "        [3.3540],\n",
       "        [3.1862],\n",
       "        [3.8257],\n",
       "        [2.7416],\n",
       "        [4.2225],\n",
       "        [2.7477],\n",
       "        [3.6237],\n",
       "        [4.5200],\n",
       "        [4.2299],\n",
       "        [1.2363],\n",
       "        [3.7014],\n",
       "        [4.1780],\n",
       "        [4.1721],\n",
       "        [1.2363],\n",
       "        [4.4213],\n",
       "        [3.7892],\n",
       "        [3.2070],\n",
       "        [4.2970],\n",
       "        [4.2223],\n",
       "        [3.8379],\n",
       "        [3.9929],\n",
       "        [4.2683],\n",
       "        [3.9755],\n",
       "        [4.2758],\n",
       "        [4.2649],\n",
       "        [4.1171],\n",
       "        [4.3804],\n",
       "        [3.4015],\n",
       "        [2.2122],\n",
       "        [4.1368],\n",
       "        [3.1378],\n",
       "        [2.3084],\n",
       "        [4.1519],\n",
       "        [4.0646],\n",
       "        [1.2363],\n",
       "        [3.9871],\n",
       "        [4.1291],\n",
       "        [3.9365],\n",
       "        [4.0811],\n",
       "        [3.9921],\n",
       "        [3.9921],\n",
       "        [3.7405],\n",
       "        [2.2122],\n",
       "        [3.9408],\n",
       "        [4.2773],\n",
       "        [2.2122],\n",
       "        [3.2981],\n",
       "        [3.6763],\n",
       "        [3.9691],\n",
       "        [1.2363],\n",
       "        [4.2213],\n",
       "        [2.2122],\n",
       "        [4.4035],\n",
       "        [3.3359],\n",
       "        [1.2363],\n",
       "        [4.0653],\n",
       "        [3.8607],\n",
       "        [3.9876],\n",
       "        [3.5908],\n",
       "        [2.7394],\n",
       "        [3.8760],\n",
       "        [3.9945],\n",
       "        [4.4410],\n",
       "        [4.2773],\n",
       "        [4.2315],\n",
       "        [3.0741],\n",
       "        [2.3085],\n",
       "        [2.3087],\n",
       "        [3.8449],\n",
       "        [2.2122],\n",
       "        [3.9977],\n",
       "        [3.0345],\n",
       "        [3.6764],\n",
       "        [4.3770],\n",
       "        [3.5999],\n",
       "        [4.0573],\n",
       "        [3.4804],\n",
       "        [3.9921],\n",
       "        [4.1656],\n",
       "        [4.0764],\n",
       "        [4.1988],\n",
       "        [4.0803],\n",
       "        [4.1645],\n",
       "        [3.7333],\n",
       "        [4.2649],\n",
       "        [3.2035],\n",
       "        [4.0594],\n",
       "        [3.1102],\n",
       "        [4.0286],\n",
       "        [4.1054],\n",
       "        [4.2127],\n",
       "        [4.4103],\n",
       "        [4.1748],\n",
       "        [3.1653],\n",
       "        [3.9403],\n",
       "        [4.3563],\n",
       "        [3.9655],\n",
       "        [4.1916],\n",
       "        [3.6842],\n",
       "        [3.9697],\n",
       "        [4.3157],\n",
       "        [3.9895],\n",
       "        [4.2970],\n",
       "        [3.6550],\n",
       "        [4.0745],\n",
       "        [1.2363],\n",
       "        [4.2721],\n",
       "        [3.7403],\n",
       "        [2.2122],\n",
       "        [2.2122],\n",
       "        [3.0206],\n",
       "        [3.8372],\n",
       "        [3.7813],\n",
       "        [3.3578],\n",
       "        [3.7347],\n",
       "        [3.5036],\n",
       "        [2.6761],\n",
       "        [3.0005],\n",
       "        [3.4696],\n",
       "        [3.1742],\n",
       "        [3.9929],\n",
       "        [4.4275],\n",
       "        [4.4415],\n",
       "        [4.0641],\n",
       "        [3.3240],\n",
       "        [1.2363],\n",
       "        [4.0672],\n",
       "        [4.0516],\n",
       "        [4.0885],\n",
       "        [4.3375],\n",
       "        [4.0831],\n",
       "        [4.0004],\n",
       "        [3.6743],\n",
       "        [4.3587],\n",
       "        [3.7463],\n",
       "        [2.7593],\n",
       "        [4.0567],\n",
       "        [2.3086],\n",
       "        [4.0192],\n",
       "        [3.6764],\n",
       "        [4.1842],\n",
       "        [4.2682],\n",
       "        [4.1148],\n",
       "        [3.9913],\n",
       "        [4.0293],\n",
       "        [4.3475],\n",
       "        [3.4800],\n",
       "        [4.1721],\n",
       "        [4.3622],\n",
       "        [3.5918],\n",
       "        [2.3081],\n",
       "        [4.0636],\n",
       "        [3.0741],\n",
       "        [4.3243],\n",
       "        [3.9325],\n",
       "        [2.9718],\n",
       "        [4.5138],\n",
       "        [3.4448],\n",
       "        [3.3213],\n",
       "        [1.2363],\n",
       "        [2.9528],\n",
       "        [4.0678],\n",
       "        [3.1420],\n",
       "        [3.6673],\n",
       "        [3.8729],\n",
       "        [4.2460],\n",
       "        [3.4263],\n",
       "        [4.3935],\n",
       "        [3.0575],\n",
       "        [3.8581],\n",
       "        [3.8407],\n",
       "        [3.5733],\n",
       "        [4.0406],\n",
       "        [3.8562],\n",
       "        [3.6764],\n",
       "        [3.3166],\n",
       "        [4.0393],\n",
       "        [4.3429],\n",
       "        [2.6761],\n",
       "        [4.0877],\n",
       "        [4.1766],\n",
       "        [4.1841],\n",
       "        [4.4335],\n",
       "        [4.2897],\n",
       "        [3.1649],\n",
       "        [3.0744],\n",
       "        [3.2915],\n",
       "        [4.4415],\n",
       "        [3.7427],\n",
       "        [1.2363],\n",
       "        [1.2363],\n",
       "        [3.0698],\n",
       "        [4.0510],\n",
       "        [4.3638],\n",
       "        [3.1518],\n",
       "        [3.9273],\n",
       "        [3.9004],\n",
       "        [4.0716],\n",
       "        [3.8712],\n",
       "        [3.9917],\n",
       "        [3.0150],\n",
       "        [1.2363],\n",
       "        [4.3713],\n",
       "        [4.3080],\n",
       "        [4.1712],\n",
       "        [2.3081],\n",
       "        [1.2363],\n",
       "        [3.9274],\n",
       "        [3.9409],\n",
       "        [3.9153],\n",
       "        [4.0834],\n",
       "        [3.0768],\n",
       "        [4.2555],\n",
       "        [3.0715],\n",
       "        [3.0047],\n",
       "        [4.0171],\n",
       "        [3.9890],\n",
       "        [1.2363],\n",
       "        [1.2363],\n",
       "        [3.9442],\n",
       "        [4.2710],\n",
       "        [2.2122],\n",
       "        [4.3432],\n",
       "        [3.6407],\n",
       "        [3.9550],\n",
       "        [4.1818],\n",
       "        [3.9348],\n",
       "        [1.2363],\n",
       "        [3.3293],\n",
       "        [3.3540],\n",
       "        [3.6255],\n",
       "        [4.4665],\n",
       "        [4.0770],\n",
       "        [4.3175],\n",
       "        [3.6395],\n",
       "        [4.2800],\n",
       "        [3.6172],\n",
       "        [3.5908],\n",
       "        [4.4023],\n",
       "        [4.2973],\n",
       "        [2.3082],\n",
       "        [2.2122],\n",
       "        [4.2551],\n",
       "        [4.2785],\n",
       "        [3.3578],\n",
       "        [4.1667],\n",
       "        [3.8647],\n",
       "        [4.1519],\n",
       "        [3.9030],\n",
       "        [3.2460],\n",
       "        [4.2766],\n",
       "        [3.7492],\n",
       "        [4.3821],\n",
       "        [4.3480],\n",
       "        [1.2363],\n",
       "        [4.0842],\n",
       "        [3.5908],\n",
       "        [4.0588],\n",
       "        [2.2122],\n",
       "        [4.3049],\n",
       "        [4.2016],\n",
       "        [3.6764],\n",
       "        [4.2831],\n",
       "        [3.6138],\n",
       "        [3.7741],\n",
       "        [4.4919],\n",
       "        [3.6584],\n",
       "        [3.5312],\n",
       "        [3.2340],\n",
       "        [4.2759],\n",
       "        [3.9767],\n",
       "        [2.7468],\n",
       "        [4.2300],\n",
       "        [4.2997],\n",
       "        [3.9521],\n",
       "        [2.7386],\n",
       "        [4.0900],\n",
       "        [2.3086],\n",
       "        [3.9837],\n",
       "        [4.5206],\n",
       "        [4.4665],\n",
       "        [4.2317],\n",
       "        [3.2449],\n",
       "        [3.7427],\n",
       "        [2.3083],\n",
       "        [3.5940],\n",
       "        [3.8479],\n",
       "        [3.4152],\n",
       "        [2.7396],\n",
       "        [1.2363],\n",
       "        [3.1381],\n",
       "        [4.2755],\n",
       "        [3.9161],\n",
       "        [1.2363],\n",
       "        [3.8027],\n",
       "        [3.8379],\n",
       "        [3.9940],\n",
       "        [4.1712],\n",
       "        [1.2363],\n",
       "        [3.9342],\n",
       "        [4.4035],\n",
       "        [3.2065],\n",
       "        [4.2773],\n",
       "        [3.9153],\n",
       "        [1.2363],\n",
       "        [3.4729],\n",
       "        [4.0016],\n",
       "        [3.9909],\n",
       "        [1.2363],\n",
       "        [3.6064],\n",
       "        [4.1846],\n",
       "        [3.0744],\n",
       "        [3.0706],\n",
       "        [4.1269],\n",
       "        [3.2516],\n",
       "        [3.5846],\n",
       "        [3.9565],\n",
       "        [3.6002],\n",
       "        [3.0706],\n",
       "        [3.8327],\n",
       "        [3.6764],\n",
       "        [4.3618],\n",
       "        [3.8975],\n",
       "        [2.7522],\n",
       "        [4.2208],\n",
       "        [4.4924],\n",
       "        [2.3079],\n",
       "        [2.3081],\n",
       "        [1.2363],\n",
       "        [3.9929],\n",
       "        [4.4141],\n",
       "        [3.4719],\n",
       "        [2.3087],\n",
       "        [3.2060],\n",
       "        [4.1931],\n",
       "        [4.0651],\n",
       "        [3.6236],\n",
       "        [3.2449],\n",
       "        [4.1886],\n",
       "        [4.0286],\n",
       "        [3.9569],\n",
       "        [4.3699],\n",
       "        [4.1721],\n",
       "        [3.6749],\n",
       "        [3.7347],\n",
       "        [3.8673],\n",
       "        [4.2390],\n",
       "        [4.0715],\n",
       "        [4.3617],\n",
       "        [4.1258],\n",
       "        [4.4038],\n",
       "        [3.9929],\n",
       "        [3.7361],\n",
       "        [1.2363],\n",
       "        [4.2649],\n",
       "        [4.0920],\n",
       "        [3.1410],\n",
       "        [4.1558],\n",
       "        [3.5455],\n",
       "        [4.0798],\n",
       "        [3.7326],\n",
       "        [3.7515],\n",
       "        [3.0739],\n",
       "        [4.3984],\n",
       "        [4.3507],\n",
       "        [3.7339],\n",
       "        [3.6271],\n",
       "        [1.2363],\n",
       "        [3.7865],\n",
       "        [4.4674],\n",
       "        [3.8379],\n",
       "        [3.8375],\n",
       "        [4.0834],\n",
       "        [4.3194],\n",
       "        [4.1407],\n",
       "        [3.2979],\n",
       "        [3.5352],\n",
       "        [4.1225],\n",
       "        [3.5836],\n",
       "        [4.0192],\n",
       "        [4.3175],\n",
       "        [1.2363],\n",
       "        [4.1028],\n",
       "        [4.2841],\n",
       "        [2.7584],\n",
       "        [2.9856],\n",
       "        [2.2122],\n",
       "        [4.0601],\n",
       "        [3.1469],\n",
       "        [4.1614],\n",
       "        [4.3431],\n",
       "        [3.0137],\n",
       "        [3.7463],\n",
       "        [3.9190],\n",
       "        [1.2363],\n",
       "        [3.5658],\n",
       "        [4.3852],\n",
       "        [4.4053],\n",
       "        [1.2363],\n",
       "        [4.1946],\n",
       "        [4.1341],\n",
       "        [3.9921],\n",
       "        [3.8306],\n",
       "        [4.1153],\n",
       "        [4.0185],\n",
       "        [4.3093],\n",
       "        [4.3925],\n",
       "        [3.6319],\n",
       "        [3.5061],\n",
       "        [3.6911],\n",
       "        [3.9847],\n",
       "        [4.0623],\n",
       "        [4.0348],\n",
       "        [3.5724],\n",
       "        [4.1179],\n",
       "        [4.2501],\n",
       "        [3.1755],\n",
       "        [4.2314],\n",
       "        [3.5888],\n",
       "        [3.9921],\n",
       "        [3.9118],\n",
       "        [1.2363],\n",
       "        [4.2946],\n",
       "        [4.2645],\n",
       "        [1.2363],\n",
       "        [4.2773],\n",
       "        [3.6171],\n",
       "        [3.3943],\n",
       "        [4.0761],\n",
       "        [3.6310],\n",
       "        [3.1606],\n",
       "        [3.8379],\n",
       "        [4.2970],\n",
       "        [3.8647],\n",
       "        [3.9624],\n",
       "        [4.0041],\n",
       "        [4.0879],\n",
       "        [3.6764],\n",
       "        [3.6754],\n",
       "        [3.0744],\n",
       "        [4.1150],\n",
       "        [1.2363],\n",
       "        [3.5891],\n",
       "        [3.6764],\n",
       "        [4.2649],\n",
       "        [2.2122],\n",
       "        [4.4186],\n",
       "        [3.8861],\n",
       "        [4.2410],\n",
       "        [1.2363],\n",
       "        [4.2679],\n",
       "        [2.7575],\n",
       "        [4.2570],\n",
       "        [3.1725],\n",
       "        [4.1805],\n",
       "        [3.8347],\n",
       "        [4.1783],\n",
       "        [1.2363],\n",
       "        [3.6764],\n",
       "        [2.7584],\n",
       "        [4.3040],\n",
       "        [3.0963],\n",
       "        [3.7785],\n",
       "        [3.7250],\n",
       "        [4.1852],\n",
       "        [4.3187],\n",
       "        [4.2876],\n",
       "        [4.1538],\n",
       "        [4.1231],\n",
       "        [2.7382],\n",
       "        [3.9408],\n",
       "        [3.9289],\n",
       "        [4.1938],\n",
       "        [2.3081],\n",
       "        [4.4025],\n",
       "        [4.2773],\n",
       "        [4.2773],\n",
       "        [2.3086],\n",
       "        [4.2758],\n",
       "        [4.4388],\n",
       "        [4.0868],\n",
       "        [4.0570],\n",
       "        [3.8373],\n",
       "        [1.2363],\n",
       "        [4.0786],\n",
       "        [4.3270],\n",
       "        [2.2122],\n",
       "        [4.0091],\n",
       "        [3.8385],\n",
       "        [3.8013],\n",
       "        [3.3712],\n",
       "        [4.0532],\n",
       "        [4.3535],\n",
       "        [3.6170],\n",
       "        [4.2522],\n",
       "        [4.2206],\n",
       "        [4.1247],\n",
       "        [3.4630],\n",
       "        [1.2363]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLE_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
